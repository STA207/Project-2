---
title: "Effects of Class Size on Math Performance in Tennessee"
output:
  html_document:
    df_print: paged
  pdf_document:
    df_print: paged
    fig_caption: yes
    number_sections: yes
---

<style type="text/css">

body{ /* Normal  */
      font-size: 18px;
  }

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,message=FALSE,warning=FALSE)
```

```{r, include=FALSE}
library("AER")
library("foreign")
library("sqldf")
library("plotly")
library("kableExtra")
```


```{r, include=FALSE}
setwd("/Users/rongkui/Desktop/Classes/STA207/Project_2/")
star = read.spss("STAR_Students.sav",to.data.frame = TRUE)
```


```{r, include = FALSE}
data1 = names(star) %in% c("g1tchid","g1classtype","g1schid","g1surban", "g1classsize","g1tmathss", "g1tgen")
data1 = star[,data1]
data1 = data1[complete.cases(data1),]
```





```{r, include = FALSE}
#Group by teachers as unit
t_data1 = sqldf("SELECT DISTINCT(g1tchid) as g1_teachID,
                            g1schid as g1_schoolID,
                            g1classtype as g1_classtype,
                            g1surban as g1_surburban,
                            COUNT(g1tchid) as g1_classsize,
                            AVG(g1tmathss) as g1_avgmath,
                            MIN(g1tmathss) as g1_minmath,
                            MAX(g1tmathss) as g1_maxmath,
                            MEDIAN(g1tmathss) as g1_medmath
                    FROM data1
                    GROUP BY g1tchid")


t_data1$g1_teachID = as.factor(t_data1$g1_teachID)
t_data1$g1_schoolID = as.factor(t_data1$g1_schoolID)
t_data1$g1_classtype= as.factor(t_data1$g1_classtype)
t_data1$g1_surburban = as.factor(t_data1$g1_surburban)



#write.csv(t_data1, "Teach_as_Unit.csv")
```

```{r, include = FALSE}
#Boxplot of average class math scores broken by class type
box1 = plot_ly(t_data1, y = ~g1_avgmath, color = ~g1_classtype, type = "box") %>%
  layout(title = "Average Classroom Score",
         yaxis = list(title = "Classroom Average Sclaed Math Score"),
         xaxis = list(title = "Class Type"))

#Boxplot of min class math scores broken by class type
box2 = plot_ly(t_data1, y = ~g1_minmath, color = ~g1_classtype, type = "box") %>%
  layout(title = "Minimum Classroom Score",
         yaxis = list(title = "Classroom Minimum Sclaed Math Score"),
         xaxis = list(title = "Class Type")) 

#Boxplot of min class math scores broken by class type
box3 = plot_ly(t_data1, y = ~g1_maxmath, color = ~g1_classtype, type = "box") %>%
  layout(title = "Classroom Maximum Score",
         yaxis = list(title = "Sclaed Math Score"),
         xaxis = list(title = "Class Type"))

#Boxplot of med class math scores broken by class type
box4 = plot_ly(t_data1, y = ~g1_medmath, color = ~g1_classtype, type = "box") %>%
  layout(title = "Classroom Median Score",
         yaxis = list(title = "Sclaed Math Score"),
         xaxis = list(title = "Class Type"))

#Boxplot of med class math scores broken by schoolID
box5 = plot_ly(t_data1, y = ~g1_medmath, color = ~g1_schoolID, type = "box") %>%
  layout(title = "Classroom Median Score",
         yaxis = list(title = "Sclaed Math Score"),
         xaxis = list(title = "SchoolID"))
```


```{r, include = FALSE}
#Histograms for Average Scores
a = sqldf("SELECT g1_avgmath FROM t_data1
          WHERE g1_classtype == 'SMALL CLASS'")

b = sqldf("SELECT g1_avgmath FROM t_data1
          WHERE g1_classtype  == 'REGULAR + AIDE CLASS'")

c = sqldf("SELECT g1_avgmath FROM t_data1
          WHERE g1_classtype == 'REGULAR CLASS'")

hist_avgsmall <- plot_ly(alpha = 0.2) %>%
  add_histogram(x = ~a$g1_avgmath, marker = list(color = "#C6CFE5")) %>%
  layout(barmode = "overlay",
         title = "Class Average Scaled Math Score",
         xaxis = list(title = "SMALL CLASSES"))

hist_avgreg <- plot_ly(alpha = 0.2) %>%
  add_histogram(x = ~b$g1_avgmath, marker = list(color = "#FDC6B0")) %>%
  layout(barmode = "overlay",
         title = "Class Average Scaled Math Score",
         xaxis = list(title = "REGULAR CLASSES"))

hist_avgregaide <- plot_ly(alpha = 0.2) %>%
  add_histogram(x = ~c$g1_avgmath, marker = list(color = "#B2E0D2")) %>%
  layout(barmode = "overlay",
         title = "Class Average Scaled Math Score",
         xaxis = list(title = "REGULAR + AIDE CLASSES"))

```

```{r, include = FALSE}
#Histograms for Average Scores
a = sqldf("SELECT g1_minmath FROM t_data1
          WHERE g1_classtype == 'SMALL CLASS'")

b = sqldf("SELECT g1_minmath FROM t_data1
          WHERE g1_classtype  == 'REGULAR + AIDE CLASS'")

c = sqldf("SELECT g1_minmath FROM t_data1
          WHERE g1_classtype == 'REGULAR CLASS'")

hist_minsmall <- plot_ly(alpha = 0.2) %>%
  add_histogram(x = ~a$g1_minmath, marker = list(color = "#C6CFE5")) %>%
  layout(barmode = "overlay",
         title = "Class Minimum Scaled Math Score",
         xaxis = list(title = "SMALL CLASSES"))

hist_minreg <- plot_ly(alpha = 0.2) %>%
  add_histogram(x = ~b$g1_minmath, marker = list(color = "#FDC6B0")) %>%
  layout(barmode = "overlay",
         title = "Class Minimum Scaled Math Score",
         xaxis = list(title = "REGULAR CLASSES"))

hist_minregaide <- plot_ly(alpha = 0.2) %>%
  add_histogram(x = ~c$g1_minmath, marker = list(color = "#B2E0D2")) %>%
  layout(barmode = "overlay",
         title = "Class Minimum Scaled Math Score",
         xaxis = list(title = "REGULAR + AIDE CLASSES"))

```



```{r, include = FALSE}
Lv_test_avg = leveneTest(g1_avgmath~g1_classtype, data = t_data1) #Equal Variacnes
kable(Lv_test_avg, caption = "Levene's Test for Equal Variance: Classroom Average Score") %>% kable_styling(bootstrap_options = "striped", full_width = F)

Lv_test_min = leveneTest(g1_avgmath~g1_classtype, data = t_data1) #Equal Variacnes
kable(Lv_test_min, caption = "Levene's Test for Equal Variance: Classroom Minimum Score") %>% kable_styling(bootstrap_options = "striped", full_width = F)
```



```{r, include = FALSE}
#ANOVA

avg_anova = aov(g1_avgmath ~ g1_classtype + g1_schoolID + g1_classtype:g1_schoolID, 
                  data = t_data1)

summary(avg_anova)
plot(avg_anova) #normality is an issue


min_anova = aov(g1_minmath ~ g1_classtype + g1_schoolID + g1_classtype:g1_schoolID, 
                  data = t_data1)
summary(min_anova)
plot(min_anova) #normality is NOT an issue


med_anova = aov(g1_medmath ~ g1_classtype + g1_schoolID + g1_classtype:g1_schoolID, 
                  data = t_data1)
summary(med_anova)
plot(med_anova)
plot(avg_anova)#normality is NOT an issue
```

***

### Team ID: Team 6

#### NAME: Connor Rosenberg
#### NAME: Rongkui Han
#### NAME: Yuqing Yang
#### NAME: Nassim Ali-Chaouche

***

# Introduction


## Background

The Student/Teacher Achievement Ratio (STAR) was a four-year longitudinal class-size study funded by the Tennessee General Assembly and conducted by the State Department of Education. Over 7,000 students from kindergarten to 3rd grade in 79 schools were randomly assigned into one of three interventions: small class (13 to 17 students per teacher), regular class (22 to 25 students per teacher), and regular-with-aide class (22 to 25 students with a full-time teacher's aide). Classroom teachers were also randomly assigned to the classes they would teach. The interventions were initiated as the students entered school in kindergarten and continued through third grade.     

Some of the key features of project STAR are:    
1. *All Tennessee schools with K-3 classes were invited to participate.* Giving every school a chance to join the study helped ensure a diverse sample as well as rule out the possibility that class-size effects could be attributed to selection bias.   

2. *Each school included in the study had to have a large enough student body to form as least one of each of the three class types.* The within-school design provided built-in control for differences among schools in terms of resources, leadership, and facilities.   

3. *Schools from inner-city, urban, suburban and rural locations were included in the experiment.* This feature guaranteed that samples would include children from various ethnic backgrounds and income levels.   

4. *Students and teachers were randomly assigned to their class type.*    

5. *Investigators followed the standard procedures for confidentiality in human subjects' research.*   

6. *No children were to receive fewer services than normal because of the experiment.*    

7. *Student achievement was to be tracked by standarized tests, which were carefully monitored.*   


## Questions of Interest

Our questions of interest are as follows:     

1. Is there a significant difference in a first-grade teacher's average math scores across the three different class sizes?    
2. Are teacher's performances relatively stable between different schools? That is, does the school itself affect class average math scores?    
3. Does our ANOVA model fit well with the data? In other words, are the analysis of variance assumptions satisfied?    
4. Can we draw causal conclusion that class sizes affect the class average math scores of first-grade teachers?      

# Analysis Plan

## Population and study design

Project STAR is an example of stratified randomized design, where experimental units are grouped together according to certain pre-treatment characteristics into strata. Within each stratum, a completely randomized experiment is conducted. In this study, each school can be viewed as a stratum. A two-way ANOVA test is fitting for answering our questions of interest under the stratified randomized design. One factor in the ANOVA model will be class size, whose main effect is of primary interest in this study. The other factor will be school ID, in order to control for and observe the stratum effect. 

To expand on our previous findings, we will set teachers as the experimental unit, rather than the individual student. We will use the *median* scaled 1st grade math score of all students under each teacher for our analysis. The median score of a class truthfully reflects the class' performance. In additon, the median is usually a more robust summary statistic than the mean, because it is less affected by outliers. The adjustment of experimental unit will enable us to make a causal statement as to the effect of class size on educational outcome.   

## Statistical Analysis

### Descriptive Analysis

> Task 1: Explore math scaled scores in the 1st with teachers as the unit. Generate summary statistics (in forms of tables or plots) that you find informative, and explain them.    

### Main Analysis

For our analysis, we will construct the following factor effects model for the classroom median math score.

$$y_{ijk} = μ_{..} + τ_i + β_j + (τβ)_{ij} + ε_{ ijk}$$
for:
$$i  \epsilon[1,2,3,4]$$
$$j  \epsilon[1,2,...,76]$$
Where:

$$\sum_{i=1}^{4} τ_i = 0$$
$$\sum_{i=1}^{76} β_j = 0$$
and 
- $μ_{..}$ represents the overall classroom median score across all treatment levels.   
- $τ_i$ represents the effect of each class size on the overall median math score.   
- $β_j$ represents the effect of each school on the overall median math score.    
- $(τβ)_{ij}$ represents the interaction effect, if any, of each school & class size combination on the overall median math score.    

Our model is appropriate to answer the questions of interest because it captures the effect of each treatment on the class’ median performance while controlling for other external factors by blocking by school id.

Figure #, which shows the boxplots of the classroom median scores for each school, highlights the variability in teacher performance across each distinct school. This variability is likely due to the similar demographic features within schools, but various demographic features between them. For example, schools located in areas of high affluence may achieve better classroom performance since more students have access to academic support in addition to greater parent oversight. Similarly, schools who pull their students from less affluent areas may see worse classroom performance due to student food insecurity, lack of academic support, and other social deficiencies. 

```{r, echo=FALSE}
box5
```

Because of this high variability in median classroom performance between schools, blocking by school id in our model helps to extract the true effect of the class size treatment on the classroom median performance. 

### Sensitivity Analysis

# Results

## Descriptive Analysis

## Main Analysis

```{r, echo=FALSE}
med_anova_int = aov(g1_medmath ~ g1_classtype + g1_schoolID + g1_classtype:g1_schoolID, 
                  data = t_data1)

med_anova1_int = anova(lm(g1_medmath ~ g1_classtype + g1_schoolID + g1_classtype:g1_schoolID, 
                  data = t_data1))


med_anova1_int = round(med_anova1_int, 2)
med_anova1_int$`Pr(>F)` = c("<.001", "<.001",.39, "NA")


kable(med_anova1_int, caption  = "ANOVA Table with Interaction") %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

> Task 5: Conduct model diagnostic and/or sensitivity analysis.

> Task 6: Test whether there is a difference in math scaled score in 1st grade across teachers in different class types. Justify your choice of test.

## Sensitivity analysis

# Discussion

In this report, we presented our usage of 2-way ANOVA to analyze the effect of class size on first-grade teachers' teaching performance in math in a stratified randomized experiment, using each school as a stratum. We explored the effect of including schoo-by-class size interactions in our model, and concluded that interactions between the two factors did not contribute significantly to the variance partitioning of the data. Model diagnostics suggested that the dataset satisfied assumptions for ANOVA. Results derived from the fitting of the model suggested significant difference in a first-grade teacher's median math scores across different class sizes. Pairwise comparisons suggested that **small classes and regular classes with aides both outperformed regular classes without aides**. The model also revealed significant performance differences across schools, with the largest pairwaise difference being **XXX** in class median 1st grade scale math score.      

This analysis enables us to make causal statements regarding the effect of class size on teacher's performance in math education. This is made possible by using teachers as experimental units, thus satisfying the SUTVA and independence assumptions necessary for causal inferences:    

**SUTVA**:
Definition: *The potential outcomes for any unit do not vary with the treatments assigned to other units, and, for each unit, there are no different forms or versions of each treatment level, which lead to different potential outcomes.*     
The experimental unit used in the analysis first satisfies the no-interference component of SUTVA – the assumption that the treatment
applied to one unit does not affect the outcome for other units. On the basis of prior knowledge of school systems, it is realistic to assume that one teacher being assigned to a specific class size does not affect the teaching outcoome of another teacher. The second component of SUTVA requires that individuals receiving a specific treatment cannot receive different forms of that treatment. In our case, due to the strict randomization implemented in the experiment, the class taught by one teacher is by nature homogenous with a class taught by another.       

**Independence Assumption**:    
Definition: *the assignment of treatment is independent of potential outcomes of experimental units.*   
This assumption is met in the experiment by using double randomization: One random assignment is that of teachers to classes. The second randomization is of students to classes/teachers. The design ensures that high/low performance teacher or students were not systematically enriched in any class-size treatments. In light of this, systematic effects can be interpreted as the effects of class size. 

Therefore, our analysis concludes that smaller class size has a positive average causal effect on a teacher's teaching outcome in math. 
This is different from the conclusion of Project I. SUTVA was not plausible when using individual students as experimental units. Interactions between students likely resulted in altered potential outcome of one student due to the treatment assigned to another, thus violating SUTVA. In that case, rejections of the null hypothesis would not necessarily be convincing evidence of effects of class size; it may simply indicate the presence of peer effects. In contrast, using teachers as experimental units does not rely on no-interference assumptions among students. This makes the results reported here credible evidence of causal class-size effects.


